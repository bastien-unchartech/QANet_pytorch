{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load preproc.py\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import ujson as json\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from codecs import open\n",
    "\n",
    "'''\n",
    "The content of this file is mostly copied from https://github.com/HKUST-KnowComp/R-Net/blob/master/prepro.py\n",
    "'''\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "\n",
    "def word_tokenize(sent):\n",
    "    doc = nlp(sent)\n",
    "    return [token.text for token in doc]\n",
    "\n",
    "\n",
    "def convert_idx(text, tokens):\n",
    "    current = 0\n",
    "    spans = []\n",
    "    for token in tokens:\n",
    "        current = text.find(token, current)\n",
    "        if current < 0:\n",
    "            print(\"Token {} cannot be found\".format(token))\n",
    "            raise Exception()\n",
    "        spans.append((current, current + len(token)))\n",
    "        current += len(token)\n",
    "    return spans\n",
    "\n",
    "\n",
    "def process_file(filename, data_type, word_counter, char_counter):\n",
    "    print(\"Generating {} examples...\".format(data_type))\n",
    "    examples = []\n",
    "    eval_examples = {}\n",
    "    total = 0\n",
    "    with open(filename, \"r\") as fh:\n",
    "        source = json.load(fh)\n",
    "        for article in tqdm(source[\"data\"]):\n",
    "            for para in article[\"paragraphs\"]:\n",
    "                context = para[\"context\"].replace(\n",
    "                    \"''\", '\" ').replace(\"``\", '\" ')\n",
    "                context_tokens = word_tokenize(context)\n",
    "                context_chars = [list(token) for token in context_tokens]\n",
    "                spans = convert_idx(context, context_tokens)\n",
    "                for token in context_tokens:\n",
    "                    word_counter[token] += len(para[\"qas\"])\n",
    "                    for char in token:\n",
    "                        char_counter[char] += len(para[\"qas\"])\n",
    "                        \n",
    "                for qa in para[\"qas\"]:\n",
    "                    total += 1\n",
    "                    ques = qa[\"question\"].replace(\n",
    "                        \"''\", '\" ').replace(\"``\", '\" ')\n",
    "                    ques_tokens = word_tokenize(ques)\n",
    "                    ques_chars = [list(token) for token in ques_tokens]\n",
    "                    for token in ques_tokens:\n",
    "                        word_counter[token] += 1\n",
    "                        for char in token:\n",
    "                            char_counter[char] += 1\n",
    "                    y1s, y2s = [], []\n",
    "                    answer_texts = []\n",
    "                    for answer in qa[\"answers\"]:\n",
    "                        answer_text = answer[\"text\"]\n",
    "                        answer_start = answer['answer_start']\n",
    "                        answer_end = answer_start + len(answer_text)\n",
    "                        answer_texts.append(answer_text)\n",
    "                        answer_span = []\n",
    "                        for idx, span in enumerate(spans):\n",
    "                            if not (answer_end <= span[0] or answer_start >= span[1]):\n",
    "                                answer_span.append(idx)\n",
    "                        y1, y2 = answer_span[0], answer_span[-1]\n",
    "                        y1s.append(y1)\n",
    "                        y2s.append(y2)\n",
    "                    example = {\"context_tokens\": context_tokens, \"context_chars\": context_chars,\n",
    "                               \"ques_tokens\": ques_tokens,\n",
    "                               \"ques_chars\": ques_chars, \"y1s\": y1s, \"y2s\": y2s, \"id\": total}\n",
    "                    examples.append(example)\n",
    "                    eval_examples[str(total)] = {\n",
    "                        \"context\": context, \"spans\": spans, \"answers\": answer_texts, \"uuid\": qa[\"id\"]}\n",
    "        print(\"{} questions in total\".format(len(examples)))\n",
    "    return examples, eval_examples\n",
    "\n",
    "\n",
    "def get_embedding(counter, data_type, limit=-1, emb_file=None, vec_size=None):\n",
    "    print(\"Generating {} embedding...\".format(data_type))\n",
    "    embedding_dict = {}\n",
    "    filtered_elements = [k for k, v in counter.items() if v > limit]\n",
    "    if emb_file is not None:\n",
    "        assert vec_size is not None\n",
    "        with open(emb_file, \"r\", encoding=\"utf-8\") as fh:\n",
    "            for line in tqdm(fh):\n",
    "                array = line.split()\n",
    "                word = \"\".join(array[0:-vec_size])\n",
    "                vector = list(map(float, array[-vec_size:]))\n",
    "                if word in counter and counter[word] > limit:\n",
    "                    embedding_dict[word] = vector\n",
    "        print(\"{} / {} tokens have corresponding {} embedding vector\".format(\n",
    "            len(embedding_dict), len(filtered_elements), data_type))\n",
    "    else:\n",
    "        assert vec_size is not None\n",
    "        for token in filtered_elements:\n",
    "            embedding_dict[token] = [np.random.normal(\n",
    "                scale=0.1) for _ in range(vec_size)]\n",
    "        print(\"{} tokens have corresponding embedding vector\".format(\n",
    "            len(filtered_elements)))\n",
    "\n",
    "    NULL = \"--NULL--\"\n",
    "    OOV = \"--OOV--\"\n",
    "    token2idx_dict = {token: idx for idx, token in enumerate(embedding_dict.keys(), 2)}\n",
    "    token2idx_dict[NULL] = 0\n",
    "    token2idx_dict[OOV] = 1\n",
    "    embedding_dict[NULL] = [0. for _ in range(vec_size)]\n",
    "    embedding_dict[OOV] = [0. for _ in range(vec_size)]\n",
    "    idx2emb_dict = {idx: embedding_dict[token]\n",
    "                    for token, idx in token2idx_dict.items()}\n",
    "    emb_mat = [idx2emb_dict[idx] for idx in range(len(idx2emb_dict))]\n",
    "    return emb_mat, token2idx_dict\n",
    "\n",
    "\n",
    "def convert_to_features(config, data, word2idx_dict, char2idx_dict):\n",
    "    example = {}\n",
    "    context, question = data\n",
    "    context = context.replace(\"''\", '\" ').replace(\"``\", '\" ')\n",
    "    question = question.replace(\"''\", '\" ').replace(\"``\", '\" ')\n",
    "    example['context_tokens'] = word_tokenize(context)\n",
    "    example['ques_tokens'] = word_tokenize(question)\n",
    "    example['context_chars'] = [list(token) for token in example['context_tokens']]\n",
    "    example['ques_chars'] = [list(token) for token in example['ques_tokens']]\n",
    "\n",
    "    para_limit = config.para_limit\n",
    "    ques_limit = config.ques_limit\n",
    "    ans_limit = config.ans_limit\n",
    "    char_limit = config.char_limit\n",
    "\n",
    "    def filter_func(example):\n",
    "        return len(example[\"context_tokens\"]) > para_limit or \\\n",
    "               len(example[\"ques_tokens\"]) > ques_limit\n",
    "\n",
    "    if filter_func(example):\n",
    "        raise ValueError(\"Context/Questions lengths are over the limit\")\n",
    "\n",
    "    context_idxs = np.zeros([para_limit], dtype=np.int32)\n",
    "    context_char_idxs = np.zeros([para_limit, char_limit], dtype=np.int32)\n",
    "    ques_idxs = np.zeros([ques_limit], dtype=np.int32)\n",
    "    ques_char_idxs = np.zeros([ques_limit, char_limit], dtype=np.int32)\n",
    "    y1 = np.zeros([para_limit], dtype=np.float32)\n",
    "    y2 = np.zeros([para_limit], dtype=np.float32)\n",
    "\n",
    "    def _get_word(word):\n",
    "        for each in (word, word.lower(), word.capitalize(), word.upper()):\n",
    "            if each in word2idx_dict:\n",
    "                return word2idx_dict[each]\n",
    "        return 1\n",
    "\n",
    "    def _get_char(char):\n",
    "        if char in char2idx_dict:\n",
    "            return char2idx_dict[char]\n",
    "        return 1\n",
    "\n",
    "    for i, token in enumerate(example[\"context_tokens\"]):\n",
    "        context_idxs[i] = _get_word(token)\n",
    "\n",
    "    for i, token in enumerate(example[\"ques_tokens\"]):\n",
    "        ques_idxs[i] = _get_word(token)\n",
    "\n",
    "    for i, token in enumerate(example[\"context_chars\"]):\n",
    "        for j, char in enumerate(token):\n",
    "            if j == char_limit:\n",
    "                break\n",
    "            context_char_idxs[i, j] = _get_char(char)\n",
    "\n",
    "    for i, token in enumerate(example[\"ques_chars\"]):\n",
    "        for j, char in enumerate(token):\n",
    "            if j == char_limit:\n",
    "                break\n",
    "            ques_char_idxs[i, j] = _get_char(char)\n",
    "\n",
    "    return context_idxs, context_char_idxs, ques_idxs, ques_char_idxs\n",
    "\n",
    "\n",
    "def build_features(config, examples, data_type, out_file, word2idx_dict, char2idx_dict, is_test=False):\n",
    "    para_limit = config.para_limit\n",
    "    ques_limit = config.ques_limit\n",
    "    ans_limit = config.ans_limit\n",
    "    char_limit = config.char_limit\n",
    "\n",
    "    def filter_func(example, is_test=False):\n",
    "        return len(example[\"context_tokens\"]) > para_limit or \\\n",
    "               len(example[\"ques_tokens\"]) > ques_limit or \\\n",
    "               (example[\"y2s\"][0] - example[\"y1s\"][0]) > ans_limit\n",
    "\n",
    "    print(\"Processing {} examples...\".format(data_type))\n",
    "    total = 0\n",
    "    total_ = 0\n",
    "    meta = {}\n",
    "    N = len(examples)\n",
    "    context_idxs = []\n",
    "    context_char_idxs = []\n",
    "    ques_idxs = []\n",
    "    ques_char_idxs = []\n",
    "    y1s = []\n",
    "    y2s = []\n",
    "    ids = []\n",
    "    for n, example in tqdm(enumerate(examples)):\n",
    "        total_ += 1\n",
    "\n",
    "        if filter_func(example, is_test):\n",
    "            continue\n",
    "\n",
    "        total += 1\n",
    "\n",
    "        def _get_word(word):\n",
    "            for each in (word, word.lower(), word.capitalize(), word.upper()):\n",
    "                if each in word2idx_dict:\n",
    "                    return word2idx_dict[each]\n",
    "            return 1\n",
    "\n",
    "        def _get_char(char):\n",
    "            if char in char2idx_dict:\n",
    "                return char2idx_dict[char]\n",
    "            return 1\n",
    "\n",
    "        context_idx = np.zeros([para_limit], dtype=np.int32)\n",
    "        context_char_idx = np.zeros([para_limit, char_limit], dtype=np.int32)\n",
    "        ques_idx = np.zeros([ques_limit], dtype=np.int32)\n",
    "        ques_char_idx = np.zeros([ques_limit, char_limit], dtype=np.int32)\n",
    "\n",
    "        for i, token in enumerate(example[\"context_tokens\"]):\n",
    "            context_idx[i] = _get_word(token)\n",
    "        context_idxs.append(context_idx)\n",
    "\n",
    "        for i, token in enumerate(example[\"ques_tokens\"]):\n",
    "            ques_idx[i] = _get_word(token)\n",
    "        ques_idxs.append(ques_idx)\n",
    "\n",
    "        for i, token in enumerate(example[\"context_chars\"]):\n",
    "            for j, char in enumerate(token):\n",
    "                if j == char_limit:\n",
    "                    break\n",
    "                context_char_idx[i, j] = _get_char(char)\n",
    "        context_char_idxs.append(context_char_idx)\n",
    "\n",
    "        for i, token in enumerate(example[\"ques_chars\"]):\n",
    "            for j, char in enumerate(token):\n",
    "                if j == char_limit:\n",
    "                    break\n",
    "                ques_char_idx[i, j] = _get_char(char)\n",
    "        ques_char_idxs.append(ques_char_idx)\n",
    "\n",
    "        start, end = example[\"y1s\"][-1], example[\"y2s\"][-1]\n",
    "        y1s.append(start)\n",
    "        y2s.append(end)\n",
    "        ids.append(example[\"id\"])\n",
    "\n",
    "    np.savez(out_file, context_idxs=np.array(context_idxs), context_char_idxs=np.array(context_char_idxs),\n",
    "             ques_idxs=np.array(ques_idxs), ques_char_idxs=np.array(ques_char_idxs), y1s=np.array(y1s),\n",
    "             y2s=np.array(y2s), ids=np.array(ids))\n",
    "    print(\"Built {} / {} instances of features in total\".format(total, total_))\n",
    "    meta[\"total\"] = total\n",
    "    return meta\n",
    "\n",
    "\n",
    "def save(filename, obj, message=None):\n",
    "    if message is not None:\n",
    "        print(\"Saving {}...\".format(message))\n",
    "        with open(filename, \"w\") as fh:\n",
    "            json.dump(obj, fh)\n",
    "\n",
    "\n",
    "def preproc(config):\n",
    "    word_counter, char_counter = Counter(), Counter()\n",
    "    #train_examples, train_eval = process_file(config.train_file, \"train\", word_counter, char_counter)\n",
    "    #dev_examples, dev_eval = process_file(config.dev_file, \"dev\", word_counter, char_counter)\n",
    "    # test_examples, test_eval = process_file(config.test_file, \"test\", word_counter, char_counter)\n",
    "\n",
    "    # My own\n",
    "    train_examples, train_eval = process_file('data/dev-v1.1.json', \"train\", word_counter, char_counter)\n",
    "    \n",
    "    word_emb_file = config.fasttext_file if config.fasttext else config.glove_word_file\n",
    "    char_emb_file = config.glove_char_file if config.pretrained_char else None\n",
    "    char_emb_size = config.glove_char_size if config.pretrained_char else None\n",
    "    char_emb_dim = config.glove_dim if config.pretrained_char else config.char_dim\n",
    "\n",
    "    word_emb_mat, word2idx_dict = get_embedding(\n",
    "        word_counter, \"word\", emb_file=word_emb_file, vec_size=config.glove_dim)\n",
    "    char_emb_mat, char2idx_dict = get_embedding(\n",
    "        char_counter, \"char\", emb_file=char_emb_file, vec_size=char_emb_dim)\n",
    "\n",
    "    build_features(config, train_examples, \"train\", config.train_record_file, word2idx_dict, char2idx_dict)\n",
    "    dev_meta = build_features(config, dev_examples, \"dev\", config.dev_record_file, word2idx_dict, char2idx_dict)\n",
    "    # test_meta = build_features(config, test_examples, \"test\", config.test_record_file, word2idx_dict, char2idx_dict, is_test=True)\n",
    "\n",
    "    save(config.word_emb_file, word_emb_mat, message=\"word embedding\")\n",
    "    save(config.char_emb_file, char_emb_mat, message=\"char embedding\")\n",
    "    save(config.train_eval_file, train_eval, message=\"train eval\")\n",
    "    save(config.dev_eval_file, dev_eval, message=\"dev eval\")\n",
    "    # save(config.test_eval_file, test_eval, message=\"test eval\")\n",
    "    save(config.word2idx_file, word2idx_dict, message=\"word dictionary\")\n",
    "    save(config.char2idx_file, char2idx_dict, message=\"char dictionary\")\n",
    "    save(config.dev_meta, dev_meta, message=\"dev meta\")\n",
    "    # save(config.test_meta, test_meta, message=\"test meta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
