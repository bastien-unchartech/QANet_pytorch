{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load main.py\n",
    "from config import config, device\n",
    "from preproc import preproc\n",
    "from absl import app\n",
    "from math import log2\n",
    "import os\n",
    "import numpy as np\n",
    "import ujson as json\n",
    "import re\n",
    "from collections import Counter\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.cuda\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "'''\n",
    "Some functions are from the official evaluation script.\n",
    "'''\n",
    "\n",
    "\n",
    "class SQuADDataset(Dataset):\n",
    "    def __init__(self, npz_file, num_steps, batch_size):\n",
    "        super().__init__()\n",
    "        data = np.load(npz_file)\n",
    "        self.context_idxs = torch.from_numpy(data[\"context_idxs\"]).long()\n",
    "        self.context_char_idxs = torch.from_numpy(data[\"context_char_idxs\"]).long()\n",
    "        self.ques_idxs = torch.from_numpy(data[\"ques_idxs\"]).long()\n",
    "        self.ques_char_idxs = torch.from_numpy(data[\"ques_char_idxs\"]).long()\n",
    "        self.y1s = torch.from_numpy(data[\"y1s\"]).long()\n",
    "        self.y2s = torch.from_numpy(data[\"y2s\"]).long()\n",
    "        self.ids = torch.from_numpy(data[\"ids\"]).long()\n",
    "        num = len(self.ids)\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps if num_steps >= 0 else num // batch_size\n",
    "        num_items = num_steps * batch_size\n",
    "        idxs = list(range(num))\n",
    "        self.idx_map = []\n",
    "        i, j = 0, num\n",
    "\n",
    "        while j <= num_items:\n",
    "            random.shuffle(idxs)\n",
    "            self.idx_map += idxs.copy()\n",
    "            i = j\n",
    "            j += num\n",
    "        random.shuffle(idxs)\n",
    "        self.idx_map += idxs[:num_items - i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_steps\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        idxs = torch.LongTensor(self.idx_map[item:item + self.batch_size])\n",
    "        res = (self.context_idxs[idxs],\n",
    "               self.context_char_idxs[idxs],\n",
    "               self.ques_idxs[idxs],\n",
    "               self.ques_char_idxs[idxs],\n",
    "               self.y1s[idxs],\n",
    "               self.y2s[idxs], self.ids[idxs])\n",
    "        return res\n",
    "\n",
    "\n",
    "class EMA(object):\n",
    "    def __init__(self, decay):\n",
    "        self.decay = decay\n",
    "        self.shadows = {}\n",
    "        self.devices = {}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.shadows)\n",
    "\n",
    "    def get(self, name: str):\n",
    "        return self.shadows[name].to(self.devices[name])\n",
    "\n",
    "    def set(self, name: str, param: nn.Parameter):\n",
    "        self.shadows[name] = param.data.to('cpu').clone()\n",
    "        self.devices[name] = param.data.device\n",
    "\n",
    "    def update_parameter(self, name: str, param: nn.Parameter):\n",
    "        if name in self.shadows:\n",
    "            data = param.data\n",
    "            new_shadow = self.decay * data + (1.0 - self.decay) * self.get(name)\n",
    "            param.data.copy_(new_shadow)\n",
    "            self.shadows[name] = new_shadow.to('cpu').clone()\n",
    "\n",
    "\n",
    "def convert_tokens(eval_file, qa_id, pp1, pp2):\n",
    "    answer_dict = {}\n",
    "    remapped_dict = {}\n",
    "    for qid, p1, p2 in zip(qa_id, pp1, pp2):\n",
    "        context = eval_file[str(qid)][\"context\"]\n",
    "        spans = eval_file[str(qid)][\"spans\"]\n",
    "        uuid = eval_file[str(qid)][\"uuid\"]\n",
    "        l = len(spans)\n",
    "        if p1 >= l or p2 >= l:\n",
    "            ans = \"\"\n",
    "        else:\n",
    "            start_idx = spans[p1][0]\n",
    "            end_idx = spans[p2][1]\n",
    "            ans = context[start_idx: end_idx]\n",
    "        answer_dict[str(qid)] = ans\n",
    "        remapped_dict[uuid] = ans\n",
    "    return answer_dict, remapped_dict\n",
    "\n",
    "\n",
    "def evaluate(eval_file, answer_dict):\n",
    "    f1 = exact_match = total = 0\n",
    "    for key, value in answer_dict.items():\n",
    "        total += 1\n",
    "        ground_truths = eval_file[key][\"answers\"]\n",
    "        prediction = value\n",
    "        exact_match += metric_max_over_ground_truths(exact_match_score, prediction, ground_truths)\n",
    "        f1 += metric_max_over_ground_truths(f1_score, prediction, ground_truths)\n",
    "    exact_match = 100.0 * exact_match / total\n",
    "    f1 = 100.0 * f1 / total\n",
    "    return {'exact_match': exact_match, 'f1': f1}\n",
    "\n",
    "\n",
    "def normalize_answer(s):\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "\n",
    "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
    "    scores_for_ground_truths = []\n",
    "    for ground_truth in ground_truths:\n",
    "        score = metric_fn(prediction, ground_truth)\n",
    "        scores_for_ground_truths.append(score)\n",
    "    return max(scores_for_ground_truths)\n",
    "\n",
    "\n",
    "def train(model, optimizer, scheduler, ema, dataset, start, length):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for i in tqdm(range(start, length + start), total=length):\n",
    "        optimizer.zero_grad()\n",
    "        Cwid, Ccid, Qwid, Qcid, y1, y2, ids = dataset[i]\n",
    "        Cwid, Ccid, Qwid, Qcid = Cwid.to(device), Ccid.to(device), Qwid.to(device), Qcid.to(device)\n",
    "        p1, p2 = model(Cwid, Ccid, Qwid, Qcid)\n",
    "        y1, y2 = y1.to(device), y2.to(device)\n",
    "        loss1 = F.nll_loss(p1, y1, size_average=True)\n",
    "        loss2 = F.nll_loss(p2, y2, size_average=True)\n",
    "        loss = (loss1 + loss2) / 2\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        for name, p in model.named_parameters():\n",
    "            if p.requires_grad: ema.update_parameter(name, p)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
    "    loss_avg = np.mean(losses)\n",
    "    print(\"STEP {:8d} loss {:8f}\\n\".format(i + 1, loss_avg))\n",
    "\n",
    "\n",
    "def valid(model, dataset, eval_file):\n",
    "    model.eval()\n",
    "    answer_dict = {}\n",
    "    losses = []\n",
    "    num_batches = config.val_num_batches\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(random.sample(range(0, len(dataset)), num_batches), total=num_batches):\n",
    "            Cwid, Ccid, Qwid, Qcid, y1, y2, ids = dataset[i]\n",
    "            Cwid, Ccid, Qwid, Qcid = Cwid.to(device), Ccid.to(device), Qwid.to(device), Qcid.to(device)\n",
    "            p1, p2 = model(Cwid, Ccid, Qwid, Qcid)\n",
    "            y1, y2 = y1.to(device), y2.to(device)\n",
    "            loss1 = F.nll_loss(p1, y1, size_average=True)\n",
    "            loss2 = F.nll_loss(p2, y2, size_average=True)\n",
    "            loss = (loss1 + loss2) / 2\n",
    "            losses.append(loss.item())\n",
    "            yp1 = torch.argmax(p1, 1)\n",
    "            yp2 = torch.argmax(p2, 1)\n",
    "            yps = torch.stack([yp1, yp2], dim=1)\n",
    "            ymin, _ = torch.min(yps, 1)\n",
    "            ymax, _ = torch.max(yps, 1)\n",
    "            answer_dict_, _ = convert_tokens(eval_file, ids.tolist(), ymin.tolist(), ymax.tolist())\n",
    "            answer_dict.update(answer_dict_)\n",
    "    loss = np.mean(losses)\n",
    "    metrics = evaluate(eval_file, answer_dict)\n",
    "    metrics[\"loss\"] = loss\n",
    "    print(\"VALID loss {:8f} F1 {:8f} EM {:8f}\\n\".format(loss, metrics[\"f1\"],\\\n",
    "                                                        metrics[\"exact_match\"]))\n",
    "\n",
    "\n",
    "def test(model, dataset, eval_file):\n",
    "    model.eval()\n",
    "    answer_dict = {}\n",
    "    losses = []\n",
    "    num_batches = config.test_num_batches\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(num_batches), total=num_batches):\n",
    "            Cwid, Ccid, Qwid, Qcid, y1, y2, ids = dataset[i]\n",
    "            Cwid, Ccid, Qwid, Qcid = Cwid.to(device), Ccid.to(device), Qwid.to(device), Qcid.to(device)\n",
    "            p1, p2 = model(Cwid, Ccid, Qwid, Qcid)\n",
    "            y1, y2 = y1.to(device), y2.to(device)\n",
    "            loss1 = F.nll_loss(p1, y1, size_average=True)\n",
    "            loss2 = F.nll_loss(p2, y2, size_average=True)\n",
    "            loss = (loss1 + loss2) / 2\n",
    "            losses.append(loss.item())\n",
    "            yp1 = torch.argmax(p1, 1)\n",
    "            yp2 = torch.argmax(p2, 1)\n",
    "            yps = torch.stack([yp1, yp2], dim=1)\n",
    "            ymin, _ = torch.min(yps, 1)\n",
    "            ymax, _ = torch.max(yps, 1)\n",
    "            answer_dict_, _ = convert_tokens(eval_file, ids.tolist(), ymin.tolist(), ymax.tolist())\n",
    "            answer_dict.update(answer_dict_)\n",
    "    loss = np.mean(losses)\n",
    "    metrics = evaluate(eval_file, answer_dict)\n",
    "    f = open(\"log/answers.json\", \"w\")\n",
    "    json.dump(answer_dict, f)\n",
    "    f.close()\n",
    "    metrics[\"loss\"] = loss\n",
    "    print(\"TEST loss {:8f} F1 {:8f} EM {:8f}\\n\".format(loss, metrics[\"f1\"], metrics[\"exact_match\"]))\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def train_entry(config):\n",
    "    from models import QANet\n",
    "\n",
    "    with open(config.word_emb_file, \"r\") as fh:\n",
    "        word_mat = np.array(json.load(fh), dtype=np.float32)\n",
    "    with open(config.char_emb_file, \"r\") as fh:\n",
    "        char_mat = np.array(json.load(fh), dtype=np.float32)\n",
    "    with open(config.train_eval_file, \"r\") as fh:\n",
    "        train_eval_file = json.load(fh)\n",
    "    with open(config.dev_eval_file, \"r\") as fh:\n",
    "        dev_eval_file = json.load(fh)\n",
    "\n",
    "    print(\"Building model...\")\n",
    "\n",
    "    train_dataset = SQuADDataset(config.train_record_file, config.num_steps, config.batch_size)\n",
    "    dev_dataset = SQuADDataset(config.dev_record_file, config.test_num_batches, config.batch_size)\n",
    "\n",
    "    lr = config.learning_rate\n",
    "    base_lr = 1.0\n",
    "    warm_up = config.lr_warm_up_num\n",
    "\n",
    "    model = QANet(word_mat, char_mat).to(device)\n",
    "    ema = EMA(config.ema_decay)\n",
    "    for name, p in model.named_parameters():\n",
    "        if p.requires_grad: ema.set(name, p)\n",
    "    params = filter(lambda param: param.requires_grad, model.parameters())\n",
    "    optimizer = optim.Adam(lr=base_lr, betas=(config.beta1, config.beta2), eps=1e-7, weight_decay=3e-7, params=params)\n",
    "    cr = lr / log2(warm_up)\n",
    "    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda ee: cr * log2(ee + 1) if ee < warm_up else lr)\n",
    "    L = config.checkpoint\n",
    "    N = config.num_steps\n",
    "    best_f1 = best_em = patience = 0\n",
    "    for iter in range(0, N, L):\n",
    "        train(model, optimizer, scheduler, ema, train_dataset, iter, L)\n",
    "        valid(model, train_dataset, train_eval_file)\n",
    "        metrics = test(model, dev_dataset, dev_eval_file)\n",
    "        print(\"Learning rate: {}\".format(scheduler.get_lr()))\n",
    "        dev_f1 = metrics[\"f1\"]\n",
    "        dev_em = metrics[\"exact_match\"]\n",
    "        if dev_f1 < best_f1 and dev_em < best_em:\n",
    "            patience += 1\n",
    "            if patience > config.early_stop: break\n",
    "        else:\n",
    "            patience = 0\n",
    "            best_f1 = max(best_f1, dev_f1)\n",
    "            best_em = max(best_em, dev_em)\n",
    "\n",
    "        fn = os.path.join(config.save_dir, \"model.pt\")\n",
    "        torch.save(model, fn)\n",
    "\n",
    "\n",
    "def test_entry(config):\n",
    "    with open(config.dev_eval_file, \"r\") as fh:\n",
    "        dev_eval_file = json.load(fh)\n",
    "    dev_dataset = SQuADDataset(config.dev_record_file, -1, config.batch_size)\n",
    "    fn = os.path.join(config.save_dir, \"model.pt\")\n",
    "    model = torch.load(fn)\n",
    "    test(model, dev_dataset, dev_eval_file)\n",
    "\n",
    "\n",
    "def main(_):\n",
    "    if config.mode == \"train\":\n",
    "        train_entry(config)\n",
    "    elif config.mode == \"data\":\n",
    "        preproc(config)\n",
    "    elif config.mode == \"debug\":\n",
    "        config.batch_size = 2\n",
    "        config.num_steps = 32\n",
    "        config.test_num_batches = 2\n",
    "        config.val_num_batches = 2\n",
    "        config.checkpoint = 2\n",
    "        config.period = 1\n",
    "        train_entry(config)\n",
    "    elif config.mode == \"test\":\n",
    "        test_entry(config)\n",
    "    else:\n",
    "        print(\"Unknown mode\")\n",
    "        exit(0)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(main)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
